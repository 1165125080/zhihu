# zhihu
一个爬取知乎所有用户信息的爬虫
## 项目介绍
此爬虫是使用scrapy框架搭建的知乎全站爬虫，理论上可以获得知乎所有用户的详细信息。
## 原理介绍
根据米尔格兰姆的六度空间理论，你和任何一个陌生人之间所间隔的人不会超过六个，也就是说，最多通过五个人就能够认识
任何一个陌生人。<br>
因此可以选取几个知乎大V，获得其关注列表与粉丝列表，得到其url_token，就可以构造一个新的请求来获得该用户的详细信息，
构造另一个请求来获得该用户的关注列表与粉丝列表，如此不断迭代，就可以得到互相关注的所有用户的信息。当然，这种方式采
集不到两种用户信息，一种是零关注、零粉丝的用户，另外一种是一个自成一体的小圈子，这个圈子内的用户只是内部互相关注。
毕竟在这个联系非常紧密的世界中，这两类用户的数量很少，因此我们可以选择忽略不计。
## 运行环境
<br>解释器：python3.6 </br>
<br>爬虫框架：scrapy  </br>
<br>存储环境：MongoDB </br>
## 效率提高
由于知乎的反爬措施做的非常好，因此同一IP短时间内请求过多的话会被封号，为了提高爬取的效率，我对接了之前搭建的
[代理池](https://github.com/ZZShi/proxy_pool)，在使用知乎爬虫之前需要开启代理池，这样的话能有效的防止被封IP。<br>
目前的数据流量可以达到400条/min，相比requests效率提高了很多倍，若是需要更快的速度，可以考虑搭建一个分布式爬虫。
## 结果展示
由于时间及电脑内存限制，目前只爬取了10000+条数据，日后会对结果做一个数据分析


